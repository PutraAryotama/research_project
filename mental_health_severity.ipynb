{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1c148b",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3c690760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eccaaf",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01ef40",
   "metadata": {},
   "source": [
    "## Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9c4d8243",
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_lexicon = pd.read_csv('./data/vad/NRC-VAD-Lexicon-v2.1.txt', sep='\\t', header=0, keep_default_na=False, na_values=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20424654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>dominance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a battery</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a bit</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a bunch</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>-0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a cappella</td>\n",
       "      <td>0.134</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a couple</td>\n",
       "      <td>0.266</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54797</th>\n",
       "      <td>zorro</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54798</th>\n",
       "      <td>zucchini</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.358</td>\n",
       "      <td>-0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54799</th>\n",
       "      <td>zulu</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54800</th>\n",
       "      <td>zygote</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.333</td>\n",
       "      <td>-0.167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54801</th>\n",
       "      <td>zygotic</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54802 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             term  valence  arousal  dominance\n",
       "0       a battery    0.134   -0.298     -0.096\n",
       "1           a bit   -0.096   -0.264     -0.214\n",
       "2         a bunch    0.088   -0.350     -0.068\n",
       "3      a cappella    0.134   -0.116     -0.200\n",
       "4        a couple    0.266   -0.110      0.090\n",
       "...           ...      ...      ...        ...\n",
       "54797       zorro    0.625    0.667      0.792\n",
       "54798    zucchini    0.020   -0.358     -0.500\n",
       "54799        zulu    0.000    0.000      0.000\n",
       "54800      zygote    0.278    0.333     -0.167\n",
       "54801     zygotic    0.000   -0.500      0.000\n",
       "\n",
       "[54802 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vad_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b3234e",
   "metadata": {},
   "source": [
    "### 5-gram lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aed3360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only 5-grams lexicons\n",
    "five_grams_lexicon = vad_lexicon[vad_lexicon['term'].str.split().str.len() == 5].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ca36f",
   "metadata": {},
   "source": [
    "### 4-gram lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8917ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only 4-grams lexicons\n",
    "four_grams_lexicon = vad_lexicon[vad_lexicon['term'].str.split().str.len() == 4].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e667d8",
   "metadata": {},
   "source": [
    "### 3-gram lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5e6cbbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only 3-grams lexicons\n",
    "three_grams_lexicon = vad_lexicon[vad_lexicon['term'].str.split().str.len() == 3].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e08332",
   "metadata": {},
   "source": [
    "### 2-gram lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5dbfdc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only 2-grams lexicons\n",
    "two_grams_lexicon = vad_lexicon[vad_lexicon['term'].str.split().str.len() == 2].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e493ea3",
   "metadata": {},
   "source": [
    "### 1-gram lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "7e475a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only 1-gram lexicons\n",
    "one_gram_lexicon = vad_lexicon[vad_lexicon['term'].str.split().str.len() == 1].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95bbcde",
   "metadata": {},
   "source": [
    "## Mental Health Severity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "532e5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mental_health_severity_table = pd.read_csv('./data/BeyondBlue/commented_post_authors_edited.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a3bdbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post_ID</th>\n",
       "      <th>Author</th>\n",
       "      <th>Author_Post</th>\n",
       "      <th>Author_Last_Comment</th>\n",
       "      <th>Beyond Blue Staff</th>\n",
       "      <th>Blue Voices Member</th>\n",
       "      <th>Champion Alumni</th>\n",
       "      <th>Community Champion</th>\n",
       "      <th>Community Member</th>\n",
       "      <th>Moderator</th>\n",
       "      <th>Valued Contributor</th>\n",
       "      <th>Days_Between</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anxi-6</td>\n",
       "      <td>Amenace</td>\n",
       "      <td>Parental Anxiety. Hi Everyone, I am not quite ...</td>\n",
       "      <td>Thank you Indigo. I am seeing a counsellor ton...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Anxi-12</td>\n",
       "      <td>Whatsinaname</td>\n",
       "      <td>Horrible week. Hi everyone, I am having a horr...</td>\n",
       "      <td>Since my last post I ended up quitting my job....</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Anxi-13</td>\n",
       "      <td>Sammy</td>\n",
       "      <td>New relationship anxiety. Dear adjust need a c...</td>\n",
       "      <td>Such a good idea this too shallpass thanks for...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anxi-17</td>\n",
       "      <td>Olive83</td>\n",
       "      <td>Easy strategies for quick response. Recently m...</td>\n",
       "      <td>You are not wrong Caught between! And I did ex...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anxi-28</td>\n",
       "      <td>jordan</td>\n",
       "      <td>Vomiting, GAD, herd. Hello everyone, I have ha...</td>\n",
       "      <td>I am replying after a year. Thank you for your...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8806</th>\n",
       "      <td>Suic-1308</td>\n",
       "      <td>Apple2468</td>\n",
       "      <td>Confused looking for support. Hello Since a ch...</td>\n",
       "      <td>Thanks for sharing those links with me. Readin...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8807</th>\n",
       "      <td>Suic-1309</td>\n",
       "      <td>jujusbizarrecircus</td>\n",
       "      <td>Intrusive thoughts. For the past two years I h...</td>\n",
       "      <td>Hey Joseph, I am actually making a homebred ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8808</th>\n",
       "      <td>Suic-1310</td>\n",
       "      <td>Teegs_</td>\n",
       "      <td>I do not know how to keep living. This has bee...</td>\n",
       "      <td>Thank you for your replies, I really appreciat...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8809</th>\n",
       "      <td>Suic-1313</td>\n",
       "      <td>lizzie50</td>\n",
       "      <td>13 Reasons Why. I am not sure if anyone has se...</td>\n",
       "      <td>Thanks for the reply Mary! The people I work w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8810</th>\n",
       "      <td>Suic-1314</td>\n",
       "      <td>Gray_13</td>\n",
       "      <td>Feeling really down and struggling. Hi guys, I...</td>\n",
       "      <td>Thank you so much Zeal. I really appreciate yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8811 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Post_ID              Author  \\\n",
       "0        Anxi-6             Amenace   \n",
       "1       Anxi-12        Whatsinaname   \n",
       "2       Anxi-13               Sammy   \n",
       "3       Anxi-17             Olive83   \n",
       "4       Anxi-28              jordan   \n",
       "...         ...                 ...   \n",
       "8806  Suic-1308           Apple2468   \n",
       "8807  Suic-1309  jujusbizarrecircus   \n",
       "8808  Suic-1310              Teegs_   \n",
       "8809  Suic-1313            lizzie50   \n",
       "8810  Suic-1314             Gray_13   \n",
       "\n",
       "                                            Author_Post  \\\n",
       "0     Parental Anxiety. Hi Everyone, I am not quite ...   \n",
       "1     Horrible week. Hi everyone, I am having a horr...   \n",
       "2     New relationship anxiety. Dear adjust need a c...   \n",
       "3     Easy strategies for quick response. Recently m...   \n",
       "4     Vomiting, GAD, herd. Hello everyone, I have ha...   \n",
       "...                                                 ...   \n",
       "8806  Confused looking for support. Hello Since a ch...   \n",
       "8807  Intrusive thoughts. For the past two years I h...   \n",
       "8808  I do not know how to keep living. This has bee...   \n",
       "8809  13 Reasons Why. I am not sure if anyone has se...   \n",
       "8810  Feeling really down and struggling. Hi guys, I...   \n",
       "\n",
       "                                    Author_Last_Comment  Beyond Blue Staff  \\\n",
       "0     Thank you Indigo. I am seeing a counsellor ton...                  0   \n",
       "1     Since my last post I ended up quitting my job....                  0   \n",
       "2     Such a good idea this too shallpass thanks for...                  0   \n",
       "3     You are not wrong Caught between! And I did ex...                  0   \n",
       "4     I am replying after a year. Thank you for your...                  0   \n",
       "...                                                 ...                ...   \n",
       "8806  Thanks for sharing those links with me. Readin...                  0   \n",
       "8807  Hey Joseph, I am actually making a homebred ca...                  0   \n",
       "8808  Thank you for your replies, I really appreciat...                  0   \n",
       "8809  Thanks for the reply Mary! The people I work w...                  0   \n",
       "8810  Thank you so much Zeal. I really appreciate yo...                  0   \n",
       "\n",
       "      Blue Voices Member  Champion Alumni  Community Champion  \\\n",
       "0                      0                0                   1   \n",
       "1                      2               28                   3   \n",
       "2                      0                0                   0   \n",
       "3                      0                0                   1   \n",
       "4                      0                0                   0   \n",
       "...                  ...              ...                 ...   \n",
       "8806                   0                1                   0   \n",
       "8807                   0                0                   2   \n",
       "8808                   0                0                   0   \n",
       "8809                   0                3                   3   \n",
       "8810                   1                2                   0   \n",
       "\n",
       "      Community Member  Moderator  Valued Contributor  Days_Between  \n",
       "0                    0          0                   0             0  \n",
       "1                    8          0                   1          1765  \n",
       "2                    2          0                   0           161  \n",
       "3                    4          0                   1             8  \n",
       "4                    1          1                   0           359  \n",
       "...                ...        ...                 ...           ...  \n",
       "8806                 0          1                   0             0  \n",
       "8807                 1          1                   0             2  \n",
       "8808                 2          1                   0            30  \n",
       "8809                 3          0                   1            23  \n",
       "8810                 3          0                   0             1  \n",
       "\n",
       "[8811 rows x 12 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mental_health_severity_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f9ba2b",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239091cc",
   "metadata": {},
   "source": [
    "## Check the Maximum N-gram in 'term'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5042c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the maximum n-gram in 'term'\n",
    "max(vad_lexicon['term'].apply(lambda x: len(x.split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545633a",
   "metadata": {},
   "source": [
    "## `vad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bd8aa1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vad(\n",
    "        text, \n",
    "        five_grams_lexicon=five_grams_lexicon, \n",
    "        four_grams_lexicon=four_grams_lexicon, \n",
    "        three_grams_lexicon=three_grams_lexicon, \n",
    "        two_grams_lexicon=two_grams_lexicon, \n",
    "        one_gram_lexicon=one_gram_lexicon):\n",
    "    \"\"\"\n",
    "    Calculate Valence, Arousal, and Dominance (VAD) scores for a given text using the NRC VAD Lexicon.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): The input text to analyze.\n",
    "    lexicon (pd.DataFrame): The VAD lexicon DataFrame with columns 'term', 'valence', 'arousal', 'dominance'.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary with average 'valence', 'arousal', and 'dominance' scores.\n",
    "    \"\"\"\n",
    "    # Preprocess the text: lowercase and split into words\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Initialize lists to store VAD scores\n",
    "    valence_scores = []\n",
    "    arousal_scores = []\n",
    "    dominance_scores = []\n",
    "\n",
    "    # initialize list to store matched n-grams and set to store their positions\n",
    "    matched_ngrams = []\n",
    "    matched_positions = set()\n",
    "\n",
    "    # check for 5-grams of text in lexicon\n",
    "    five_grams = list(ngrams(words, 5))\n",
    "    five_grams = [' '.join(gram) for gram in five_grams]\n",
    "\n",
    "    # record the positions of words that are part of matched 5-grams\n",
    "    five_grams_positions = list(ngrams(range(len(words)), 5))\n",
    "    five_grams_positions = [set(pos) for pos in five_grams_positions]\n",
    "    for gram, pos in zip(five_grams, five_grams_positions):\n",
    "        match = five_grams_lexicon[five_grams_lexicon['term'] == gram]\n",
    "        if not match.empty:\n",
    "            valence_scores.append(float(match['valence'].values[0]))\n",
    "            arousal_scores.append(float(match['arousal'].values[0]))\n",
    "            dominance_scores.append(float(match['dominance'].values[0]))\n",
    "            matched_ngrams.append(gram)\n",
    "            matched_positions = matched_positions.union(pos)\n",
    "\n",
    "    # check for 4-grams of text in lexicon\n",
    "    four_grams = list(ngrams(words, 4))\n",
    "    four_grams = [' '.join(gram) for gram in four_grams]\n",
    "\n",
    "    # record the positions of words that are part of matched 4-grams\n",
    "    four_grams_positions = list(ngrams(range(len(words)), 4))\n",
    "    four_grams_positions = [set(pos) for pos in four_grams_positions]\n",
    "\n",
    "    for gram, pos in zip(four_grams, four_grams_positions):\n",
    "        if not matched_positions.isdisjoint(pos):\n",
    "            continue  # Skip if any word in the 4-gram is part of a matched n-gram\n",
    "        match = four_grams_lexicon[four_grams_lexicon['term'] == gram]\n",
    "        if not match.empty:\n",
    "            valence_scores.append(float(match['valence'].values[0]))\n",
    "            arousal_scores.append(float(match['arousal'].values[0]))\n",
    "            dominance_scores.append(float(match['dominance'].values[0]))\n",
    "            matched_ngrams.append(gram)\n",
    "            matched_positions = matched_positions.union(pos)\n",
    "    \n",
    "    # check for 3-grams of text in lexicon\n",
    "    three_grams = list(ngrams(words, 3))\n",
    "    three_grams = [' '.join(gram) for gram in three_grams]\n",
    "\n",
    "    # record the positions of words that are part of matched 3-grams\n",
    "    three_grams_positions = list(ngrams(range(len(words)), 3))\n",
    "    three_grams_positions = [set(pos) for pos in three_grams_positions]\n",
    "\n",
    "    for gram, pos in zip(three_grams, three_grams_positions):\n",
    "        if not matched_positions.isdisjoint(pos):\n",
    "            continue  # Skip if any word in the 3-gram is part of a matched n-gram\n",
    "        match = three_grams_lexicon[three_grams_lexicon['term'] == gram]\n",
    "        if not match.empty:\n",
    "            valence_scores.append(float(match['valence'].values[0]))\n",
    "            arousal_scores.append(float(match['arousal'].values[0]))\n",
    "            dominance_scores.append(float(match['dominance'].values[0]))\n",
    "            matched_ngrams.append(gram)\n",
    "            matched_positions = matched_positions.union(pos)\n",
    "    \n",
    "    # check for 2-grams of text in lexicon, excluding words already part of matched 3-grams\n",
    "    two_grams = list(ngrams(words, 2))\n",
    "    two_grams = [' '.join(gram) for gram in two_grams]\n",
    "\n",
    "    # record the positions of words that are part of matched 2-grams\n",
    "    two_grams_positions = list(ngrams(range(len(words)), 2))\n",
    "    two_grams_positions = [set(pos) for pos in two_grams_positions]\n",
    "\n",
    "    for gram, pos in zip(two_grams, two_grams_positions):\n",
    "        if not matched_positions.isdisjoint(pos):\n",
    "            continue  # Skip if any word in the 2-gram is part of a matched n-gram\n",
    "        match = two_grams_lexicon[two_grams_lexicon['term'] == gram]\n",
    "        if not match.empty:\n",
    "            valence_scores.append(float(match['valence'].values[0]))\n",
    "            arousal_scores.append(float(match['arousal'].values[0]))\n",
    "            dominance_scores.append(float(match['dominance'].values[0]))\n",
    "            matched_ngrams.append(gram)\n",
    "            matched_positions = matched_positions.union(pos)\n",
    "\n",
    "    # check for unigrams of text in lexicon, excluding words already part of matched n-grams\n",
    "    for i, word in enumerate(words):\n",
    "        if i in matched_positions:\n",
    "            continue  # Skip if the word is part of a matched n-gram\n",
    "        match = one_gram_lexicon[one_gram_lexicon['term'] == word]\n",
    "        if not match.empty:\n",
    "            valence_scores.append(float(match['valence'].values[0]))\n",
    "            arousal_scores.append(float(match['arousal'].values[0]))\n",
    "            dominance_scores.append(float(match['dominance'].values[0]))\n",
    "            matched_ngrams.append(word)\n",
    "            matched_positions.add(i)\n",
    "\n",
    "    # Calculate average scores, return None if no scores found\n",
    "    avg_valence = sum(valence_scores) / len(valence_scores) if valence_scores else None\n",
    "    avg_arousal = sum(arousal_scores) / len(arousal_scores) if arousal_scores else None\n",
    "    avg_dominance = sum(dominance_scores) / len(dominance_scores) if dominance_scores else None\n",
    "    \n",
    "    # return {\n",
    "    #     'valence': avg_valence,\n",
    "    #     'arousal': avg_arousal,\n",
    "    #     'dominance': avg_dominance\n",
    "    # }{\n",
    "    return{\n",
    "        'valence': valence_scores,\n",
    "        'arousal': arousal_scores,\n",
    "        'dominance': dominance_scores,\n",
    "        'matched_ngrams': matched_ngrams\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29140fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Parental Anxiety. Hi Everyone, I am not quite sure how to put this in words so will just type and she what comes out. I feel so much anxiety about my teenager and the choices she will make in her life. I have to allow her to make some mistakes and learn and grow but the anxiety this gives me is quite literally strangling me. How do I stop over analyzing everything a thousand times over in my brain. It's crippling me.\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = mental_health_severity_table[\"Author_Post\"].iloc[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e855dfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "25af04cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['not quite',\n",
       " 'how to',\n",
       " 'so much',\n",
       " 'have to',\n",
       " 'stop over',\n",
       " 'thousand times',\n",
       " 'parental',\n",
       " 'anxiety',\n",
       " 'hi',\n",
       " 'everyone',\n",
       " 'am',\n",
       " 'sure',\n",
       " 'put',\n",
       " 'this',\n",
       " 'words',\n",
       " 'will',\n",
       " 'just',\n",
       " 'type',\n",
       " 'and',\n",
       " 'what',\n",
       " 'out',\n",
       " 'feel',\n",
       " 'anxiety',\n",
       " 'about',\n",
       " 'teenager',\n",
       " 'and',\n",
       " 'will',\n",
       " 'make',\n",
       " 'life',\n",
       " 'allow',\n",
       " 'to',\n",
       " 'make',\n",
       " 'some',\n",
       " 'and',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'grow',\n",
       " 'but',\n",
       " 'anxiety',\n",
       " 'this',\n",
       " 'is',\n",
       " 'quite',\n",
       " 'literally',\n",
       " 'how',\n",
       " 'do',\n",
       " 'everything',\n",
       " 'over',\n",
       " 'brain',\n",
       " 'it',\n",
       " 'crippling']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vad(text)[\"matched_ngrams\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9602b937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vad(text)[\"valence\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
